apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nvidia-device-plugin-daemonset
  name: nvidia-device-plugin-daemonset
  namespace: "FILLED BY THE OPERATOR"
  annotations:
    openshift.io/scc: hostmount-anyuid
spec:
  selector:
    matchLabels:
      app: nvidia-device-plugin-daemonset
  template:
    metadata:
      labels:
        app: nvidia-device-plugin-daemonset
    spec:
      nodeSelector:
        nvidia.com/gpu.deploy.device-plugin: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      priorityClassName: system-node-critical
      serviceAccountName: nvidia-device-plugin
      initContainers:
      - image: "FILLED BY THE OPERATOR"
        name: toolkit-validation
        command: ['sh', '-c']
        args: ["until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia container stack to be setup; sleep 5; done"]
        securityContext:
          privileged: true
        volumeMounts:
          - name: run-nvidia
            mountPath: /run/nvidia
            mountPropagation: HostToContainer
      - image: "FILLED BY THE OPERATOR"
        name: config-manager-init
        command: ["config-manager"]
        env:
        - name: ONESHOT
          value: "true"
        - name: KUBECONFIG
          value: ""
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: "spec.nodeName"
        - name: NODE_LABEL
          value: "nvidia.com/device-plugin.config"
        - name: CONFIG_FILE_SRCDIR
          value: "/available-configs"
        - name: CONFIG_FILE_DST
          value: "/config/config.yaml"
        - name: DEFAULT_CONFIG
          value: ""
        - name: SEND_SIGNAL
          value: "false"
        - name: SIGNAL
          value: ""
        - name: PROCESS_TO_SIGNAL
          value: ""
      containers:
      - image: "FILLED BY THE OPERATOR"
        name: nvidia-device-plugin
        command: [bash, -c]
        args: ["[[ -f /run/nvidia/validations/host-driver-ready ]] && driver_root=/ || driver_root=/run/nvidia/driver; export NVIDIA_DRIVER_ROOT=$driver_root; exec nvidia-device-plugin;"]
        securityContext:
          privileged: true
        env:
          - name: PASS_DEVICE_SPECS
            value: "true"
          - name: FAIL_ON_INIT_ERROR
            value: "true"
          - name: DEVICE_LIST_STRATEGY
            value: envvar
          - name: DEVICE_ID_STRATEGY
            value: uuid
          - name: NVIDIA_VISIBLE_DEVICES
            value: all
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: all
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
          - name: run-nvidia
            mountPath: /run/nvidia
            mountPropagation: HostToContainer
      - image: "FILLED BY THE OPERATOR"
        name: config-manager
        command: ["config-manager"]
        securityContext:
          privileged: true
        env:
        - name: ONESHOT
          value: "false"
        - name: KUBECONFIG
          value: ""
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: "spec.nodeName"
        - name: NODE_LABEL
          value: "nvidia.com/device-plugin.config"
        - name: CONFIG_FILE_SRCDIR
          value: "/available-configs"
        - name: CONFIG_FILE_DST
          value: "/config/config.yaml"
        - name: DEFAULT_CONFIG
          value: ""
        - name: SEND_SIGNAL
          value: "true"
        - name: SIGNAL
          value: "1" # SIGHUP
        - name: PROCESS_TO_SIGNAL
          value: "nvidia-device-plugin"
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
        - name: run-nvidia
          hostPath:
            path: "/run/nvidia"
            type: Directory